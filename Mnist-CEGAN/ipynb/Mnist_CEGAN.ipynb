{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c57500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f513394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICE\"]=\"1\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfb82d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#torch.__version__ # Get PyTorch and CUDA version\n",
    "torch.cuda.is_available() # Check that CUDA works\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51cb7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5a3748",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892e1eb0a83040be95eb24917f0e7999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26deca7e87744799afb1c58f06f20099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4124e7627844463bb4ecd2776852369c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1a4ff1260c4676ad8093b1b20ab8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformation 정의하기\n",
    "data_transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 데이터를 저장할 경로 설정\n",
    "path2data = './data'\n",
    "\n",
    "# training data 불러오기\n",
    "train_data = datasets.MNIST(path2data, train=True, download=True, transform=data_transform)\n",
    "\n",
    "# MNIST test dataset 불러오기\n",
    "val_data = datasets.MNIST(path2data, train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24a5eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "\n",
    "x_train, y_train = train_data.data, train_data.targets\n",
    "x_val, y_val = val_data.data, val_data.targets\n",
    "\n",
    "\n",
    "if len(x_train.shape) == 3:\n",
    "    x_train = x_train.unsqueeze(1)\n",
    "if len(x_val.shape) == 3:\n",
    "    x_val = x_val.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6018af60",
   "metadata": {},
   "source": [
    "### 노이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2f396f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_sampler(batch_size=10, N_NOISE = 100, N_CLASS = 10):\n",
    "    bz = np.random.uniform(-1., 1., size=[batch_size, N_NOISE]).astype(np.float32)\n",
    "    idx = np.random.random_integers(0, N_CLASS - 1, size=(batch_size,))\n",
    "    by = np.zeros((batch_size, N_CLASS))\n",
    "    by[np.arange(batch_size), idx] = 1\n",
    "\n",
    "    return bz, by, idx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef4915",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9272851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, noise_shape):\n",
    "        super(Feature_Generator, self).__init__()\n",
    "        self.dense = nn.Sequential(\n",
    "                nn.Linear(noise_shape, 128 * 4 * 4),\n",
    "                nn.BatchNorm1d(128 * 4 * 4)\n",
    "            )\n",
    "            \n",
    "        self.model = nn.Sequential(\n",
    "                nn.ConvTranspose2d(128, 48, 3, stride=2, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(48),\n",
    "                nn.LeakyReLU(True),\n",
    "\n",
    "                nn.ConvTranspose2d(48, 12, 3, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(12),\n",
    "                nn.LeakyReLU(True),\n",
    "\n",
    "                nn.ConvTranspose2d(12, 6, 4, stride=1, padding=0, bias=False),\n",
    "                nn.Tanh(),\n",
    "                nn.BatchNorm2d(6)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, einsum):\n",
    "        x = self.dense(x)\n",
    "        x = x.view(x.size(0), 512, 4, 4)\n",
    "        x = self.model(x)\n",
    "        x = torch.einsum('aijk, ai -> aijk', x, einsum) # (batch, 256, 56, 56) * (batch, 256) -> (batch, 256, 56, 56)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5c1d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super(Feature_Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 5, stride=2, padding=2, bias=False),\n",
    "            nn.LeakyReLU(True), \n",
    "\n",
    "            nn.Conv2d(32, 64, 5, stride=2, padding=2, bias=False),\n",
    "            nn.LeakyReLU(True), \n",
    "\n",
    "            nn.Conv2d(64, 128, 5, stride=2, padding=2, bias=False),\n",
    "            nn.LeakyReLU(True)\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Linear(128 * 2 * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)       # (batch, 128, 2, 2)\n",
    "        x = x.view(x.size(0), -1)   # (batch, 128 * 2 * 2)\n",
    "        x = self.dense(x)       # (batch, 1)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27fe4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_weight=None, num_classes=10):\n",
    "        super(Feature_Extractor, self).__init__()\n",
    "        \n",
    "        if pretrained_weight is None:\n",
    "            lenet5_list = list(Lenet5(num_classes=num_classes).children())\n",
    "        else:\n",
    "            lenet5_ = Lenet5(num_classes=num_classes)\n",
    "            lenet5_.load_state_dict(torch.load(pretrained_weight, map_location='cpu'))\n",
    "            lenet5_list = list(lenet5_.children())\n",
    "            \n",
    "        self.model = nn.Sequential(\n",
    "                    # stop after first conv layer\n",
    "                    *list( lenet5_list[0] )[:3]\n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d6b2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Classifier(nn.Module):\n",
    "    def __init__(self, pretrained_weight=None, num_classes=10):\n",
    "        super(Feature_Classifier, self).__init__()\n",
    "        if pretrained_weight is None:\n",
    "            lenet5_list = list(Lenet5(num_classes=num_classes).children())\n",
    "        else:\n",
    "            lenet5_ = Lenet5(num_classes=num_classes)\n",
    "            lenet5_.load_state_dict(torch.load(pretrained_weight, map_location='cpu'))\n",
    "            lenet5_list = list(lenet5_.children())\n",
    "                \n",
    "        self.feature = nn.Sequential(\n",
    "                *list( lenet5_list[0] )[3:]\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "                *list( lenet5_list[1] )[:]\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fd8c4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet5(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5, stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(6, 16, 5, stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde4980",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd018750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import utils\n",
    "import time\n",
    "import csv\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0e0bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(self, batch_size):\n",
    "    train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_data, batch_size=batch_size)\n",
    "    return train_dl, \\\n",
    "            val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d409aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalOps(object):\n",
    "\n",
    "    def __init__(self, gpu_num, batch_size):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        tmp = str(time.time())\n",
    "        self.curtime = tmp[:9]ㄷ\n",
    "        self.code_start_time = time.time()\n",
    "        \n",
    "        def evaluate(self, extractor_weight_path, classifier_weight_path):\n",
    "            _, self.data_loader_test= DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "            print('test data loader len: %d' % ( len(self.data_loader_test) ) )\n",
    "\n",
    "            self.feature_extractor = Feature_Extractor(pretrained_weight=None, num_classes=self.num_labels)\n",
    "            self.feature_extractor.load_state_dict(torch.load(extractor_weight_path))\n",
    "            self.feature_extractor.eval()\n",
    "            self.feature_extractor.to(self.device)\n",
    "\n",
    "            self.feature_classifier = Feature_Classifier(pretrained_weight=None, num_classes=self.num_labels)\n",
    "            self.feature_classifier.load_state_dict(torch.load(classifier_weight_path))\n",
    "            self.feature_classifier.eval()\n",
    "            self.feature_classifier.to(self.device)\n",
    "\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            loss = 0.0\n",
    "            step_acc = 0.0\n",
    "            total_inputs_len = 0\n",
    "            with torch.no_grad(): #이 안에서 행해지는 모든 코드들은 미분 적용이 안 됨\n",
    "                for idx, (inputs, labels) in enumerate(self.data_loader_test):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    extracted_feature = self.feature_extractor(inputs)\n",
    "                    logits_real = self.feature_classifier(extracted_feature)\n",
    "                    _, preds = torch.max(logits_real, 1)\n",
    "                    loss += self.criterion(logits_real, labels).item() * inputs.size(0)\n",
    "\n",
    "                    corr_sum = torch.sum(preds == labels.data)\n",
    "                    step_acc += corr_sum.double()\n",
    "                    total_inputs_len += inputs.size(0)\n",
    "                    if idx % 20 == 0:\n",
    "                        print('%d / %d' % (idx, len(self.data_loader_test)))\n",
    "\n",
    "            loss /= total_inputs_len\n",
    "            step_acc /= total_inputs_len\n",
    "\n",
    "            print ('Test loss: [%.6f] accuracy: [%.4f]' % (loss, step_acc))\n",
    "            print(\"time: %.3f\" % (time.time() - self.code_start_time))\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--gpu\", type=int, default=1, help=\" 0, 1, 2 or 3 \")\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help='batch_size')\n",
    "\n",
    "    # weight path\n",
    "    parser.add_argument(\"--extractor_weight_path\", type=str, default='./feature_extractor_target_resnet_caltech.pth', help='extractor weight path')\n",
    "    parser.add_argument(\"--classifier_weight_path\", type=str, default='./feature_classifier_target_resnet_caltech.pth', help='classifier weight path')\n",
    "\n",
    "    parser.add_argument(\"--pretrained_base_model\", type=str, default='', help='pretrained_base_model')\n",
    "\n",
    "    # './checkpoint/vgg16_cifar10_source.pth'\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    test = EvalOps(gpu_num=args.gpu,mbatch_size=args.batch_size)\n",
    "\n",
    "    test.evaluate(args.extractor_weight_path, args.classifier_weight_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d34b75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import time\n",
    "import csv\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "88ddb2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with_regularization\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 897\u001b[0m\n\u001b[1;32m    892\u001b[0m     test\u001b[38;5;241m.\u001b[39mtrain_gan(generator_step\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgenerator_step, rho\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mrho, lambda_gp\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlambda_gp, alpha_extractor\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39malpha_extractor, alpha_classifier\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39malpha_classifier,\n\u001b[1;32m    893\u001b[0m                     beta_classifier\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta_classifier, num_d_iters\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_d_iters, e_loss_for_6\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39me_loss_for_6, \n\u001b[1;32m    894\u001b[0m                     step_for_3_4\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstep_for_3_4, step_for_5\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstep_for_5, \n\u001b[1;32m    895\u001b[0m                     step_for_6\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstep_for_6, pretrained_base_model\u001b[38;5;241m=\u001b[39mpretrained_base_model)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown mode"
     ]
    }
   ],
   "source": [
    "class TrainOps(object):\n",
    "\n",
    "    def __init__(self, gpu_num, batch_size, extractor_learning_rate, classifier_learning_rate, discriminator_learning_rate,\n",
    "                    generator_learning_rate, minor_class_num, minor_class_ratio, with_regularization, model_save_path):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #Learning rate\n",
    "        self.extractor_learning_rate = extractor_learning_rate\n",
    "        self.classifier_learning_rate = classifier_learning_rate\n",
    "        self.discriminator_learning_rate = discriminator_learning_rate\n",
    "        self.generator_learning_rate = generator_learning_rate\n",
    "        \n",
    "        '''self.feature_dimension = 6\n",
    "        self.resize_size = 32\n",
    "        self.crop_size = 32\n",
    "        self.noise_dim = 100'''\n",
    "        \n",
    "        self.layer_outputs_source = []\n",
    "        self.layer_outputs_target = []\n",
    "\n",
    "        self.minor_class_num = minor_class_num\n",
    "        self.minor_class_ratio = minor_class_ratio\n",
    "\n",
    "        self.with_regularization = with_regularization\n",
    "\n",
    "        tmp = str(time.time())\n",
    "        self.curtime = tmp[:9]\n",
    "\n",
    "        self.model_save_path = model_save_path\n",
    "        if not os.path.isdir(self.model_save_path):\n",
    "            os.makedirs(self.model_save_path)\n",
    "            \n",
    "        self.csv_save_path = self.model_save_path + '/csv/'\n",
    "        if not os.path.isdir(self.csv_save_path):\n",
    "            os.makedirs(self.csv_save_path)\n",
    "            \n",
    "        self.generator_attention_class = self.csv_save_path + 'generator_attention_class_' + self.curtime + '.csv'\n",
    "        self.wj_extractor_file = self.csv_save_path + 'wj_extractor_' + self.curtime + '.csv'\n",
    "        self.generator_attention_file = self.csv_save_path + 'generator_attention_' + self.curtime + '.csv'\n",
    "\n",
    "        self.cal_weight_path = self.model_save_path + '/config/'\n",
    "        if not os.path.isdir(self.cal_weight_path):\n",
    "            os.makedirs(self.cal_weight_path)\n",
    "\n",
    "        self.transpose_wj_extractor_npy = self.cal_weight_path + 'transpose_wj_extractor_%s_%s.npy' \n",
    "        self.generator_attention_npy = self.cal_weight_path + 'generator_attention_%s_%s.npy' \n",
    "        self.channel_weight_json = self.cal_weight_path + 'channel_weight_%s_%s.json'\n",
    "\n",
    "        self.optimal_attention_npy = self.model_save_path + '/optimal_attention_%s_%s_%s' \n",
    "        self.feature_extractor_target_pth = self.model_save_path + '/feature_extractor_target_%s_%s_%s.pth' \n",
    "        self.feature_classifier_target_pth = self.model_save_path + '/feature_classifier_target_%s_%s_%s.pth'\n",
    "        self.feature_generator_pth = self.model_save_path + '/feature_generator_%s_%s_%s.pth' \n",
    "        self.feature_discriminator_pth = self.model_save_path + '/feature_discriminator_%s_%s_%s.pth' \n",
    "        self.code_start_time = time.time()\n",
    "\n",
    "        \n",
    "    def calculate_weighting_feature_maps_extractor(self, extractor_model, classifier_model, layer_name, label_min=10):\n",
    "        print('weight data loader len : %d' % len(self.data_loader))\n",
    "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        channel = extractor_model.state_dict()[layer_name + '.weight'].shape[0]\n",
    "        print('channel number : %d' % channel)\n",
    "\n",
    "        labels_cnt = [0 for i in range(self.num_labels)]\n",
    "        labels_min = label_min\n",
    "\n",
    "        # calculate base, jth loss\n",
    "        total_start_time = time.time()\n",
    "        base_loss = []\n",
    "        jthfilter_loss_list = [[] for i in range(channel)]\n",
    "        class_label_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(self.data_loader):\n",
    "                step_start_time = time.time()\n",
    "                class_inputs = []\n",
    "                class_labels = []\n",
    "                for batch_idx in range(inputs.size(0)):\n",
    "                    if labels_cnt[labels[batch_idx].item()] >= labels_min:\n",
    "                        continue\n",
    "                    \n",
    "                    labels_cnt[labels[batch_idx].item()] += 1\n",
    "                    class_inputs.append(inputs[batch_idx])\n",
    "                    class_labels.append(labels[batch_idx])\n",
    "\n",
    "                if len(class_inputs) == 0:\n",
    "                    if min(labels_cnt) < labels_min:\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                class_label_list.extend(class_labels)\n",
    "                class_inputs = torch.stack(class_inputs)\n",
    "                class_labels = torch.stack(class_labels)\n",
    "                \n",
    "                class_inputs = class_inputs.to(self.device)\n",
    "                class_labels = class_labels.to(self.device)\n",
    "\n",
    "                feature_outputs = extractor_model(class_inputs)\n",
    "                classifier_outputs = classifier_model(feature_outputs)\n",
    "                base_loss.extend(criterion(classifier_outputs, class_labels).cpu())\n",
    "                self.layer_outputs_source.clear()\n",
    "                self.layer_outputs_target.clear()\n",
    "\n",
    "                for j in range(channel):\n",
    "                    j_tmp_weight = extractor_model.state_dict()[layer_name + '.weight'][j,:,:,:].clone()\n",
    "                    extractor_model.state_dict()[layer_name + '.weight'][j,:,:,:] = 0\n",
    "\n",
    "                    feature_outputs = extractor_model(class_inputs)\n",
    "                    classifier_outputs = classifier_model(feature_outputs)\n",
    "                    jthfilter_loss_list[j].extend(criterion(classifier_outputs, class_labels).cpu())\n",
    "\n",
    "                    extractor_model.state_dict()[layer_name + '.weight'][j,:,:,:] = j_tmp_weight\n",
    "                    self.layer_outputs_source.clear()\n",
    "                    self.layer_outputs_target.clear()\n",
    "                \n",
    "                print('%d step loss len : %d, time : %.5f' % (i, len(base_loss), time.time() - step_start_time))\n",
    "\n",
    "        print('total loss len : %d, class label len : %d, total time : %.5f' % (len(base_loss), len(class_label_list), time.time() - total_start_time))\n",
    "        # memory clear\n",
    "        self.layer_outputs_source.clear()\n",
    "        self.layer_outputs_target.clear()\n",
    "        return base_loss, jthfilter_loss_list, class_label_list\n",
    "\n",
    "    def calculate_weighting_feature_maps_classifier(self, extractor_model, classifier_model, layer_name):\n",
    "        print('weight data loader len : %d' % len(self.data_loader))\n",
    "\n",
    "        filter_weight = []\n",
    "        for i in range(len(layer_name)):\n",
    "            channel = classifier_model.state_dict()[layer_name[i] + '.weight'].shape[0]\n",
    "            layer_filter_weight = [0] * channel\n",
    "            filter_weight.append(layer_filter_weight)         \n",
    "            \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        since = time.time()\n",
    "        for i, (inputs, labels) in enumerate(self.data_loader):\n",
    "            if i >= 4:\n",
    "                break\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            outputs = extractor_model(inputs)\n",
    "            outputs = classifier_model(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            for name, module in classifier_model.named_modules():\n",
    "                if not name in layer_name:\n",
    "                    continue\n",
    "                layer_id = layer_name.index(name)\n",
    "                channel = classifier_model.state_dict()[name + '.weight'].shape[0]\n",
    "                for j in range(channel):\n",
    "                    tmp = classifier_model.state_dict()[name + '.weight'][j,:,:,:].clone()\n",
    "                    classifier_model.state_dict()[name + '.weight'][j,:,:,:] = 0\n",
    "                    outputs = extractor_model(inputs)\n",
    "                    outputs = classifier_model(outputs)\n",
    "                    loss1 = criterion(outputs, labels)\n",
    "                    diff = loss1 - loss\n",
    "                    diff = diff.detach().cpu().numpy().item()\n",
    "                    hist = filter_weight[layer_id][j]\n",
    "                    filter_weight[layer_id][j] = 1.0 * (i * hist + diff) / (i + 1)\n",
    "                    print('%s:%d %.4f %.4f' % (name, j, diff, filter_weight[layer_id][j]))\n",
    "                    classifier_model.state_dict()[name + '.weight'][j,:,:,:] = tmp\n",
    "                    \n",
    "            print('step %d finished' % i)\n",
    "            time_elapsed = time.time() - since\n",
    "            print('step Training complete in {:.0f}m {:.0f}s'.format(\n",
    "                time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "        json.dump(filter_weight, open(self.channel_weight_json, 'w'))\n",
    "\n",
    "    def for_hook_source(self, module, input, output):\n",
    "        self.layer_outputs_source.append(output)\n",
    "\n",
    "    def for_hook_target(self, module, input, output):\n",
    "        self.layer_outputs_target.append(output)\n",
    "\n",
    "    def register_hook(self, model, func, layer_name):\n",
    "        for name, layer in model.named_modules():\n",
    "            if name in layer_name:\n",
    "                layer.register_forward_hook(func)\n",
    "\n",
    "    def train_fc(self, feature_extractor, model):\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if not name.startswith('classifier.'):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                print(name)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                            lr = 0.01, momentum=0.9, weight_decay=1e-4)\n",
    "        num_epochs = 10\n",
    "        decay_epochs = 6 \n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = math.exp(math.log(0.1) / decay_epochs))\n",
    "        since = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            total = 0.0\n",
    "            nstep = len(self.data_loader)\n",
    "            for i, (inputs, labels) in enumerate(self.data_loader):\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                features = feature_extractor(inputs)\n",
    "                outputs = model(features)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    corr_sum = torch.sum(preds == labels.data)\n",
    "                    step_acc = corr_sum.double() / len(labels)\n",
    "                    print('step: %d/%d, loss = %.4f, top1 = %.4f' %(i, nstep, loss, step_acc))\n",
    "                    \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                total += inputs.size(0)\n",
    "                        \n",
    "            scheduler.step()\n",
    "            epoch_loss = running_loss / total\n",
    "            epoch_acc = running_corrects.double() / total\n",
    "\n",
    "            print('epoch: {:d} Loss: {:.4f} Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "            time_elapsed = time.time() - since\n",
    "            print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "                time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def channel_evaluation(self, pretrained_base_model=None):\n",
    "        print ('get weighting_feature_map')\n",
    "        \n",
    "        self.data_loader, self.data_loader_test = self.load_data_set(batch_size=self.batch_size)\n",
    "        print('data loader len: %d' % ( len(self.data_loader) ) )\n",
    "\n",
    "        # set feature extractor\n",
    "        print(pretrained_base_model)\n",
    "        self.feature_extractor = Feature_Extractor(pretrained_weight=pretrained_base_model, num_classes=self.num_labels)\n",
    "        self.feature_extractor.to(self.device)\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "        # set extractor hook for feature map\n",
    "\n",
    "        self.layer_name_extractor = 'model.0' # (6 in, 16 out)\n",
    "\n",
    "        # set feature classifier\n",
    "        self.feature_classifier = Feature_Classifier(pretrained_weight=pretrained_base_model, num_classes=self.num_labels)\n",
    "        self.feature_classifier.to(self.device)\n",
    "\n",
    "        for param in self.feature_classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.feature_classifier.eval()\n",
    "        \n",
    "        # calculate weighting source extractor feature maps\n",
    "        base_loss_extractor, jthfilter_loss_extractor, class_label_list = self.calculate_weighting_feature_maps_extractor(self.feature_extractor, \n",
    "                                                                            self.feature_classifier, layer_name=self.layer_name_extractor, label_min=10)\n",
    "        extractor_number_filter = len(jthfilter_loss_extractor)\n",
    "\n",
    "        wj_extractor = np.zeros((extractor_number_filter, len(base_loss_extractor)))\n",
    "        base_loss_extractor = np.array(base_loss_extractor)\n",
    "        print(wj_extractor.shape, base_loss_extractor.shape)\n",
    "\n",
    "        for fidx in range(extractor_number_filter):\n",
    "            wj_extractor[fidx] = base_loss_extractor - np.array(jthfilter_loss_extractor[fidx])\n",
    "        \n",
    "        transpose_wj_extractor = np.transpose(wj_extractor)\n",
    "        print(transpose_wj_extractor.shape)\n",
    "        np.save(self.transpose_wj_extractor_npy, transpose_wj_extractor.mean(axis=0))\n",
    "        \n",
    "        with open(self.wj_extractor_file, 'w') as wj_list:\n",
    "            writer = csv.writer(wj_list, delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "            for t in range(transpose_wj_extractor.shape[0]):\n",
    "                def softmax_loss(loss) : \n",
    "                    max_loss = np.max(loss) \n",
    "                    exp_loss = np.exp(loss-max_loss) \n",
    "                    sum_exp_loss = np.sum(exp_loss)\n",
    "                    result = exp_loss / sum_exp_loss\n",
    "                    return result\n",
    "\n",
    "                transpose_wj_extractor[t] = softmax_loss(transpose_wj_extractor[t])\n",
    "                writer.writerow(list(transpose_wj_extractor[t]))\n",
    "\n",
    "        # initialize attention for feature generator      \n",
    "        self.generator_attention = np.zeros((self.num_labels, self.feature_dimension))\n",
    "        labels_count  = np.zeros(self.num_labels)\n",
    "        iCnt=0\n",
    "        for class_num in class_label_list:\n",
    "            class_num = class_num.item()\n",
    "            self.generator_attention[class_num, :] = 1.0 * (self.generator_attention[class_num, :] * labels_count[class_num] + transpose_wj_extractor[iCnt, :]) / (labels_count[class_num]+1)\n",
    "            labels_count[class_num] += 1\n",
    "            iCnt+=1\n",
    "\n",
    "        np.save(self.generator_attention_npy, self.generator_attention)\n",
    "\n",
    "        # save_generator_attention\n",
    "        for filter_idx in range(self.generator_attention.shape[1]):\n",
    "            filename = self.csv_save_path  + \"/\"+ \"generator_attention_\" + str(filter_idx) + \"_\" + self.curtime + \".csv\"\n",
    "            with open(filename, 'a') as generator_attention_save:\n",
    "                writer = csv.writer(generator_attention_save, delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(list(self.generator_attention[:,filter_idx]))\n",
    "\n",
    "        # set classifier hook for feature map\n",
    "\n",
    "        self.layer_name_classifier = ['feature.0'] # (6 in, 16 out)\n",
    "\n",
    "        # calculate weighting source classifier feature maps\n",
    "        self.calculate_weighting_feature_maps_classifier(self.feature_extractor, \n",
    "                                                            self.feature_classifier, layer_name=self.layer_name_classifier)\n",
    "\n",
    "    def flatten_outputs(self, fea):\n",
    "        return torch.reshape(fea, (fea.shape[0], fea.shape[1], fea.shape[2] * fea.shape[3]))\n",
    "    \n",
    "    def extractor_att_fea_map(self, fm_src, fm_tgt):\n",
    "        fea_loss = torch.tensor(0.).to(self.device)\n",
    "        \n",
    "        b, c, h, w = fm_src.shape\n",
    "        fm_src = self.flatten_outputs(fm_src)\n",
    "        fm_tgt = self.flatten_outputs(fm_tgt)\n",
    "        div_norm = h * w\n",
    "        distance = torch.norm(fm_tgt - fm_src.detach(), 2, 2)\n",
    "        distance = c * torch.mul(self.extractor_channel_weights, distance ** 2) / (h * w)\n",
    "        fea_loss += 0.5 * torch.sum(distance)\n",
    "        return fea_loss\n",
    "\n",
    "    def reg_att_fea_map(self):\n",
    "        fea_loss = torch.tensor(0.).to(self.device)\n",
    "\n",
    "        for i, (fm_src, fm_tgt) in enumerate(zip(self.layer_outputs_source, self.layer_outputs_target)):\n",
    "            b, c, h, w = fm_src.shape\n",
    "            fm_src = self.flatten_outputs(fm_src)\n",
    "            fm_tgt = self.flatten_outputs(fm_tgt)\n",
    "            div_norm = h * w\n",
    "            distance = torch.norm(fm_tgt - fm_src.detach(), 2, 2)\n",
    "            distance = c * torch.mul(self.channel_weights[i], distance ** 2) / (h * w)\n",
    "            fea_loss += 0.5 * torch.sum(distance)\n",
    "        return fea_loss\n",
    "\n",
    "    def reg_classifier(self):\n",
    "        l2_cls = torch.tensor(0.).to(self.device)\n",
    "        for name, param in self.feature_classifier_target.named_parameters():\n",
    "            if name.startswith(self.fc):\n",
    "                l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "        return l2_cls\n",
    "\n",
    "\n",
    "    def compute_gradient_penalty(self, D, real_samples, fake_samples):\n",
    "        # Random weight term for interpolation between real and fake samples\n",
    "        alpha = torch.FloatTensor(np.random.random((real_samples.size(0), 1, 1, 1))).to(self.device)\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = D(interpolates)\n",
    "        fake = Variable(torch.FloatTensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False).to(self.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0] \n",
    "        gradients = gradients.view(gradients.size(0), -1) + 1e-16\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    def train_discriminator(self):\n",
    "        self.feature_discriminator.requires_grad_(True)\n",
    "        self.feature_extractor_target.requires_grad_(False)\n",
    "        self.feature_classifier_target.requires_grad_(False)\n",
    "        self.feature_generator.requires_grad_(False)\n",
    "\n",
    "        for inputs_tmp in self.inputs_gpu:\n",
    "            bz_rand, bz_cat, _  = utils.z_sampler(self.batch_size, self.noise_dim, self.num_labels) # (batch, noise_dim), (batch, num_labels)\n",
    "            bz_input = torch.FloatTensor(np.concatenate((bz_rand, bz_cat), axis=1))         # (batch, noise_dim + num_labels)\n",
    "            bz_cat = torch.FloatTensor(bz_cat)\n",
    "            einsum = torch.matmul(bz_cat, self.generator_attention)         # (batch, feature_dimension)\n",
    "\n",
    "            bz_input = bz_input.to(self.device)\n",
    "            einsum = einsum.to(self.device)\n",
    "            generated_feature = self.feature_generator(bz_input, einsum)\n",
    "\n",
    "            extracted_feature = self.feature_extractor_target(inputs_tmp)\n",
    "\n",
    "            logits_fake = self.feature_discriminator(generated_feature)\n",
    "            logits_real = self.feature_discriminator(extracted_feature)\n",
    "            \n",
    "            self.discriminator_optimizer.zero_grad()\n",
    "            # wgan gp\n",
    "            gradient_penalty = self.compute_gradient_penalty(self.feature_discriminator, extracted_feature, generated_feature)\n",
    "            discriminator_loss = -torch.mean(logits_real) + torch.mean(logits_fake) + self.lambda_gp * gradient_penalty\n",
    "            discriminator_loss.backward()\n",
    "\n",
    "            self.discriminator_optimizer.step()\n",
    "        return discriminator_loss.item()\n",
    "        \n",
    "    def train_generator(self):\n",
    "        self.feature_discriminator.requires_grad_(False)\n",
    "        self.feature_generator.requires_grad_(True)\n",
    "\n",
    "        bz_rand, bz_cat, _  = utils.z_sampler(self.batch_size, self.noise_dim, self.num_labels) # (batch, noise_dim), (batch, num_labels)\n",
    "        bz_input = torch.FloatTensor(np.concatenate((bz_rand, bz_cat), axis=1))  # (batch, noise_dim + num_labels)\n",
    "        bz_cat = torch.FloatTensor(bz_cat)\n",
    "        einsum = torch.matmul(bz_cat, self.generator_attention) # (batch, feature_dimension)\n",
    "\n",
    "        bz_input = bz_input.to(self.device)\n",
    "        einsum = einsum.to(self.device)\n",
    "        generated_feature = self.feature_generator(bz_input, einsum)\n",
    "\n",
    "        logits_fake = self.feature_discriminator(generated_feature)\n",
    "        \n",
    "        self.generator_optimizer.zero_grad()\n",
    "        # wgan gp\n",
    "        generator_loss = -torch.mean(logits_fake)\n",
    "        generator_loss.backward()\n",
    "\n",
    "        self.generator_optimizer.step()\n",
    "        return generator_loss.item()\n",
    "    \n",
    "    def train_generator_with_fake_feature(self):\n",
    "        bz_rand, bz_cat, fake_labels  = utils.z_sampler(self.batch_size, self.noise_dim, self.num_labels) # (batch, noise_dim), (batch, num_labels)\n",
    "        bz_input = torch.FloatTensor(np.concatenate((bz_rand, bz_cat), axis=1))  # (batch, noise_dim + num_labels)\n",
    "        bz_cat = torch.FloatTensor(bz_cat)\n",
    "        einsum = torch.matmul(bz_cat, self.generator_attention) # (batch, feature_dimension)\n",
    "\n",
    "        bz_input = bz_input.to(self.device)\n",
    "        einsum = einsum.to(self.device)\n",
    "        generated_feature = self.feature_generator(bz_input, einsum)\n",
    "        fake_labels = torch.LongTensor(fake_labels).to(self.device)\n",
    "\n",
    "        extracted_feature = self.feature_extractor_target(self.inputs_gpu[0])\n",
    "\n",
    "        # merge gen, real feature\n",
    "        merged_feature = torch.cat((generated_feature, extracted_feature), dim=0) # batch size * 2\n",
    "        merged_labels = torch.cat((fake_labels, self.labels_gpu[0]), dim=0)\n",
    "\n",
    "        self.generator_optimizer.zero_grad()\n",
    "\n",
    "        # predict fake feature\n",
    "        c_logits_merged = self.feature_classifier_target(merged_feature)\n",
    "        c_loss_merged = self.criterion(c_logits_merged, merged_labels)\n",
    "        c_loss_merged.backward()\n",
    "\n",
    "        self.generator_optimizer.step()\n",
    "\n",
    "    def train_extractor_classifier(self):\n",
    "        self.feature_extractor_target.requires_grad_(True)\n",
    "        self.feature_classifier_target.requires_grad_(True)\n",
    "        self.feature_generator.requires_grad_(False)\n",
    "\n",
    "        if self.with_regularization==True:\n",
    "            self.layer_outputs_target.clear()\n",
    "            self.layer_outputs_source.clear()\n",
    "\n",
    "        # Feature Extractor by using Regularization\n",
    "        extracted_feature_target = self.feature_extractor_target(self.inputs_gpu[0])\n",
    "        e_c_loss = self.feature_classifier_target(extracted_feature_target)\n",
    "        e_c_loss = self.criterion(e_c_loss, self.labels_gpu[0])\n",
    "\n",
    "        extracted_feature_source = self.feature_extractor_source(self.inputs_gpu[0])\n",
    "        \n",
    "        # omega1 for extractor\n",
    "        if self.with_regularization==True:\n",
    "            loss_extractor_feature = self.extractor_att_fea_map(extracted_feature_source, extracted_feature_target)\n",
    "            e_loss = e_c_loss + self.alpha_extractor * loss_extractor_feature\n",
    "        else:\n",
    "            e_loss = e_c_loss\n",
    "            loss_extractor_feature = torch.zeros(1)\n",
    "\n",
    "        # Feature Classifier by using Regularization\n",
    "        # for hook layer\n",
    "        if self.with_regularization==True:\n",
    "            self.feature_classifier_source(extracted_feature_source)\n",
    "\n",
    "        # omega1, omega2 for classifier\n",
    "        if self.with_regularization==True:\n",
    "            loss_feature = self.reg_att_fea_map()\n",
    "            loss_classifier = self.reg_classifier()\n",
    "            c_loss_real = e_c_loss + self.alpha_classifier * loss_feature + self.beta_classifier * loss_classifier\n",
    "        else:\n",
    "            c_loss_real = e_c_loss\n",
    "\n",
    "        if self.with_regularization==True:\n",
    "            loss = e_loss + c_loss_real\n",
    "        else:\n",
    "            loss = e_c_loss\n",
    "\n",
    "        self.classifier_optimizer.zero_grad()\n",
    "        self.extractor_optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.extractor_optimizer.step()\n",
    "        self.classifier_optimizer.step()\n",
    "        return e_loss.item(), c_loss_real.item(), loss_extractor_feature.item()\n",
    "\n",
    "    def train_classifier_with_fake(self):\n",
    "        if self.with_regularization==True:\n",
    "            self.layer_outputs_target.clear()\n",
    "            self.layer_outputs_source.clear()\n",
    "\n",
    "        # Feature Extractor\n",
    "        extracted_feature_target = self.feature_extractor_target(self.inputs_gpu[0])\n",
    "        e_c_logit_real = self.feature_classifier_target(extracted_feature_target)\n",
    "        e_c_loss_real = self.criterion(e_c_logit_real, self.labels_gpu[0])\n",
    "\n",
    "        # Feature Classifier by using Regularization\n",
    "        bz_rand, bz_cat, fake_labels  = utils.z_sampler(self.batch_size, self.noise_dim, self.num_labels) # (batch, noise_dim), (batch, num_labels)\n",
    "        bz_input = torch.FloatTensor(np.concatenate((bz_rand, bz_cat), axis=1))  # (batch, noise_dim + num_labels)\n",
    "        bz_cat = torch.FloatTensor(bz_cat)\n",
    "        einsum = torch.matmul(bz_cat, self.generator_attention) # (batch, feature_dimension)\n",
    "\n",
    "        bz_input = bz_input.to(self.device)\n",
    "        einsum = einsum.to(self.device)\n",
    "        generated_feature = self.feature_generator(bz_input, einsum)\n",
    "\n",
    "        fake_labels = torch.LongTensor(fake_labels).to(self.device)\n",
    "        e_c_logits_fake = self.feature_classifier_target(generated_feature)\n",
    "        e_c_loss_fake = self.criterion(e_c_logits_fake, fake_labels)\n",
    "\n",
    "        merged_feature = torch.cat((generated_feature, extracted_feature_target), dim=0) # batch size * 2\n",
    "        merged_labels = torch.cat((fake_labels, self.labels_gpu[0]), dim=0)\n",
    "\n",
    "        c_logits_merged = self.feature_classifier_target(merged_feature)\n",
    "        c_loss_merged = self.criterion(c_logits_merged, merged_labels)\n",
    "\n",
    "        # for hook layer\n",
    "        if self.with_regularization==True:\n",
    "            extracted_feature_source = self.feature_extractor_source(self.inputs_gpu[0])\n",
    "            self.feature_classifier_source(extracted_feature_source)\n",
    "\n",
    "            # omega1, omega2 for classifier\n",
    "            loss_feature = self.reg_att_fea_map()\n",
    "            loss_classifier = self.reg_classifier()\n",
    "            e_c_loss_real = e_c_loss_real + self.alpha_classifier * loss_feature + self.beta_classifier * loss_classifier\n",
    "        else:    \n",
    "            loss_feature = torch.zeros(1)\n",
    "            loss_classifier = torch.zeros(1)\n",
    "        \n",
    "        c_loss = 1.0 / 3.0 * e_c_loss_real + 1.0 / 3.0 * e_c_loss_fake + 1.0 / 3.0 * c_loss_merged\n",
    "        \n",
    "        self.classifier_optimizer.zero_grad()\n",
    "\n",
    "        c_loss.backward()\n",
    "\n",
    "        self.classifier_optimizer.step()\n",
    "\n",
    "        return c_loss.item(), e_c_loss_real.item(), e_c_loss_fake.item(), loss_feature.item(), loss_classifier.item(), c_loss_merged.item()\n",
    "\n",
    "    def update_generator_attention(self, feature_extractor_target, feature_classifier_target, layer_name_extractor):\n",
    "        # Calculate weighting feature maps for extractor\n",
    "        base_loss_extractor, jthfilter_loss_extractor, class_label_list = self.calculate_weighting_feature_maps_extractor(feature_extractor_target, feature_classifier_target, \n",
    "                                                                        layer_name=layer_name_extractor, label_min=10)\n",
    "\n",
    "        extractor_number_filter = len(jthfilter_loss_extractor)\n",
    "        wj_extractor = np.zeros((extractor_number_filter, len(base_loss_extractor)))\n",
    "        print(wj_extractor.shape)\n",
    "        base_loss_extractor = np.array(base_loss_extractor)\n",
    "\n",
    "        for fidx in range(extractor_number_filter):\n",
    "            wj_extractor[fidx] = base_loss_extractor - np.array(jthfilter_loss_extractor[fidx])\n",
    "        \n",
    "        transpose_wj_extractor = np.transpose(wj_extractor)\n",
    "        for t in range(transpose_wj_extractor.shape[0]):\n",
    "            def softmax_loss(loss) : \n",
    "                max_loss = np.max(loss) \n",
    "                exp_loss = np.exp(loss-max_loss) \n",
    "                sum_exp_loss = np.sum(exp_loss)\n",
    "                result = exp_loss / sum_exp_loss\n",
    "                return result\n",
    "            transpose_wj_extractor[t] = softmax_loss(transpose_wj_extractor[t])\n",
    "        \n",
    "        #Initialize Attention for feature generator\n",
    "        target_generator_attention = np.zeros((self.num_labels, self.feature_dimension))\n",
    "        labels_count  = np.zeros(self.num_labels)\n",
    "        iCnt=0\n",
    "        for class_num in class_label_list:\n",
    "            class_num = class_num.item()\n",
    "            target_generator_attention[class_num, :] = 1.0 * (target_generator_attention[class_num, :] * labels_count[class_num] + transpose_wj_extractor[iCnt, :]) / (labels_count[class_num]+1)\n",
    "            labels_count[class_num] += 1\n",
    "            iCnt+=1\n",
    "        \n",
    "        generator_attention_np = self.rho * self.generator_attention.numpy() + (1 - self.rho) * target_generator_attention\n",
    "\n",
    "        #save_generator_attention\n",
    "        for filter_idx in range(generator_attention_np.shape[1]):\n",
    "            filename = self.csv_save_path  + \"/\"+ \"generator_attention_\" + str(filter_idx) + \"_\" + self.curtime + \".csv\"\n",
    "            with open(filename, 'a') as generator_attention_save:\n",
    "                writer = csv.writer(generator_attention_save, delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(list(generator_attention_np[:,filter_idx]))\n",
    "\n",
    "        generator_attention_np = target_generator_attention * self.feature_dimension\n",
    "\n",
    "        for class_label in range(self.num_labels):\n",
    "            generator_attention_np[class_label, :] = np.where(generator_attention_np[class_label, :] >= 0.95, generator_attention_np[class_label, :]/self.feature_dimension, 0.0)\n",
    "        \n",
    "        # write number of zero generator attention per class\n",
    "        with open(self.generator_attention_class, 'a') as wj_list:\n",
    "            writer = csv.writer(wj_list, delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "            zero_generator_attention_list = []\n",
    "            for t in range(generator_attention_np.shape[0]):\n",
    "                zero_generator_attention_list.append(len(generator_attention_np[t][generator_attention_np[t] == 0]))\n",
    "            writer.writerow(zero_generator_attention_list)\n",
    "\n",
    "        print(len(generator_attention_np[generator_attention_np == 0]))\n",
    "        self.generator_attention = torch.FloatTensor(generator_attention_np)\n",
    "\n",
    "    def evaluate(self, step, generator_step):\n",
    "        self.feature_generator.eval()\n",
    "        self.feature_extractor_target.eval()\n",
    "        self.feature_classifier_target.eval()\n",
    "        self.feature_discriminator.eval()\n",
    "\n",
    "        loss = 0.0\n",
    "        step_acc = 0.0\n",
    "        total_inputs_len = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.data_loader_test:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                extracted_feature = self.feature_extractor_target(inputs)\n",
    "                logits_real = self.feature_classifier_target(extracted_feature)\n",
    "                _, preds = torch.max(logits_real, 1)\n",
    "                loss += self.criterion(logits_real, labels).item() * inputs.size(0)\n",
    "\n",
    "                corr_sum = torch.sum(preds == labels.data)\n",
    "                step_acc += corr_sum.double()\n",
    "                total_inputs_len += inputs.size(0)\n",
    "                # gpu memory\n",
    "                if self.with_regularization==True:\n",
    "                    self.layer_outputs_target.clear()\n",
    "                    self.layer_outputs_source.clear()\n",
    "\n",
    "        loss /= total_inputs_len\n",
    "        step_acc /= total_inputs_len\n",
    "\n",
    "        print ('Step: [%d/%d] validation loss: [%.8f] validation accuracy: [%.4f]' % (step, generator_step, loss, step_acc))\n",
    "\n",
    "        self.feature_generator.train()\n",
    "        self.feature_extractor_target.train()\n",
    "        self.feature_classifier_target.train()\n",
    "        self.feature_discriminator.train()\n",
    "        return loss, step_acc\n",
    "\n",
    "    def train_gan(self, generator_step, rho, lambda_gp, alpha_extractor,\n",
    "                    alpha_classifier, beta_classifier, num_d_iters, e_loss_for_6, \n",
    "                    step_for_3_4, step_for_5, step_for_6, pretrained_base_model):\n",
    "\n",
    "        # Loss weight for gradient penalty\n",
    "        self.rho = rho\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.alpha_extractor = alpha_extractor\n",
    "        self.alpha_classifier = alpha_classifier\n",
    "        self.beta_classifier = beta_classifier\n",
    "\n",
    "        # start gan training\n",
    "        self.data_loader, self.data_loader_test = self.load_data_set(batch_size=self.batch_size)\n",
    "\n",
    "        print('train data loader len: %d' % ( len(self.data_loader) ) )\n",
    "        print('test data loader len: %d' % ( len(self.data_loader_test) ) )\n",
    "        \n",
    "        self.feature_extractor_source = Feature_Extractor( pretrained_weight=pretrained_base_model, num_classes=self.num_labels)\n",
    "        self.feature_extractor_source.requires_grad_(False)\n",
    "        self.feature_extractor_source.to(self.device)\n",
    "        self.feature_extractor_source.eval()\n",
    "        \n",
    "        self.feature_classifier_source = Feature_Classifier(pretrained_weight=pretrained_base_model, num_classes=self.num_labels)\n",
    "        self.feature_classifier_source.requires_grad_(False)\n",
    "        self.feature_classifier_source.to(self.device)\n",
    "        self.feature_classifier_source.eval()\n",
    "        \n",
    "        self.feature_extractor_target = Feature_Extractor(pretrained_weight=pretrained_base_model, num_classes=self.num_labels)\n",
    "        self.feature_extractor_target.to(self.device)\n",
    "        self.feature_classifier_target = Feature_Classifier(pretrained_weight=pretrained_base_model, num_classes=self.num_labels)\n",
    "        self.feature_classifier_target.to(self.device)\n",
    "        \n",
    "        # set extractor layer name for feature map\n",
    "\n",
    "        self.layer_name_extractor = 'model.0' # ( 6 in,  16 out)\n",
    "        \n",
    "        if self.with_regularization == True:\n",
    "   \n",
    "            self.layer_name_classifier = ['feature.0'] # (6 in, 16 out)\n",
    "            self.register_hook(self.feature_classifier_source, self.for_hook_source, self.layer_name_classifier)\n",
    "            self.register_hook(self.feature_classifier_target, self.for_hook_target, self.layer_name_classifier)\n",
    "            \n",
    "            # set fc name\n",
    "            self.fc = 'classifier.4'\n",
    "        \n",
    "        self.feature_discriminator = Feature_Discriminator(in_channels=self.feature_dimension)\n",
    "        self.feature_discriminator.to(self.device)\n",
    "\n",
    "        self.feature_generator = Feature_Generator(noise_shape=self.noise_dim + self.num_labels)\n",
    "        self.feature_generator.to(self.device)\n",
    "\n",
    "        self.feature_generator.train()\n",
    "        self.feature_extractor_target.train()\n",
    "        self.feature_classifier_target.train()\n",
    "        self.feature_discriminator.train()\n",
    "\n",
    "        self.discriminator_optimizer = optim.Adam(self.feature_discriminator.parameters(), lr=self.discriminator_learning_rate, betas=(0.5, 0.9))\n",
    "        self.generator_optimizer = optim.Adam(self.feature_generator.parameters(), lr=self.generator_learning_rate, betas=(0.5, 0.9))\n",
    "\n",
    "        \n",
    "        self.extractor_optimizer = optim.Adam(self.feature_extractor_target.parameters(), lr=self.extractor_learning_rate, betas=(0.5, 0.9))\n",
    "        self.classifier_optimizer = optim.Adam(self.feature_classifier_target.parameters(), lr=self.classifier_learning_rate, betas=(0.5, 0.9))\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if self.with_regularization==True:\n",
    "            js = np.load(self.transpose_wj_extractor_npy)\n",
    "            js = (js - np.mean(js)) / np.std(js)\n",
    "            cw = torch.from_numpy(js).float().to(self.device)\n",
    "            cw = F.softmax(cw / 5).detach()\n",
    "            self.extractor_channel_weights = cw\n",
    "            print(self.extractor_channel_weights.size())\n",
    "        \n",
    "        self.generator_attention = np.load(self.generator_attention_npy)\n",
    "        self.generator_attention = self.generator_attention * self.feature_dimension\n",
    "\n",
    "        for class_label in range(self.num_labels):\n",
    "            self.generator_attention[class_label,:]=np.where(self.generator_attention[class_label,:]>= 0.9, self.generator_attention[class_label,:]/self.feature_dimension, 0.0)\n",
    "        print(self.generator_attention)\n",
    "        self.generator_attention = torch.FloatTensor(self.generator_attention)\n",
    "        print(self.generator_attention.size())\n",
    "\n",
    "        if self.with_regularization==True:\n",
    "            self.channel_weights = []\n",
    "            channel_wei = self.channel_weight_json\n",
    "            if channel_wei:\n",
    "                for js in json.load(open(channel_wei)):\n",
    "                    js = np.array(js)\n",
    "                    js = (js - np.mean(js)) / np.std(js)\n",
    "                    cw = torch.from_numpy(js).float().to(self.device)\n",
    "                    cw = F.softmax(cw / 5).detach()\n",
    "                    self.channel_weights.append(cw)\n",
    "\n",
    "        best_step_acc = 0.0\n",
    "        step = 0\n",
    "\n",
    "        self.inputs_gpu = []\n",
    "        self.labels_gpu = []\n",
    "        for d_step in range(num_d_iters):\n",
    "            inputs, labels = next(iter(self.data_loader))\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            self.inputs_gpu.append(inputs)\n",
    "            self.labels_gpu.append(labels)\n",
    "        \n",
    "        while step < generator_step:\n",
    "            # 1. Train feature discriminator\n",
    "            d_loss = self.train_discriminator()\n",
    "            \n",
    "            # 2. train generator\n",
    "            g_loss = self.train_generator()\n",
    "\n",
    "            if step % step_for_3_4 == 0:\n",
    "                #3. Train generator using classifier with fake feature\n",
    "                self.train_generator_with_fake_feature()\n",
    "\n",
    "                #4. Train Extractor and Classifier by using BR with real features\n",
    "                e_loss, c_loss_real, omega1_weight_extractor = self.train_extractor_classifier()\n",
    "\n",
    "            if step % step_for_5 == 0 and step != 0:\n",
    "                #5. Train Classifier by using BR with real and generated features\n",
    "                c_loss, c_loss_real, c_loss_fake, omega1_weight_classifier, omega2_weight_classifier, c_loss_merged \\\n",
    "                                                                                     = self.train_classifier_with_fake()\n",
    "            \n",
    "            if step % step_for_6 == 0 and step != 0 and e_loss < e_loss_for_6:\n",
    "                #6. Updating attention for feature generator\n",
    "                self.update_generator_attention(self.feature_extractor_target, self.feature_classifier_target, self.layer_name_extractor)\n",
    "            \n",
    "            if step % 100 == 0 and step !=0:\n",
    "                #7. print current step loss, write validation Log\n",
    "                print ('Step: [%d/%d] d_loss: %.5f g_loss: %.5f c_loss: %.5f c_loss_real: %.5f c_loss_fake: %.5f e_loss: %.5f c_loss_merged: %.5f' \\\n",
    "                                        %(step, generator_step, d_loss, g_loss, c_loss, c_loss_real, c_loss_fake, e_loss, c_loss_merged))\n",
    "                print ('Step: [%d/%d] Omega1 Extractor Loss: %.5f, Omega1 Classifier Loss: %.5f, Omega2 Classifier Loss: %.5f, best accuracy: %.5f' \\\n",
    "                                            %(step, generator_step, omega1_weight_extractor, omega1_weight_classifier, omega2_weight_classifier, best_step_acc))\n",
    "                validation_loss, step_acc = self.evaluate(step, generator_step)\n",
    "                test_time = time.time()\n",
    "                print(\"time: %.3f\" % (test_time - self.code_start_time))\n",
    "                if step_acc > best_step_acc:\n",
    "                    best_step_acc = step_acc\n",
    "                    print('best accuracy: %.4f' % best_step_acc)\n",
    "\n",
    "\n",
    "            step += 1\n",
    "            self.inputs_gpu.pop(0)\n",
    "            self.labels_gpu.pop(0)\n",
    "            inputs, labels = next(iter(self.data_loader))\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            self.inputs_gpu.append(inputs)\n",
    "            self.labels_gpu.append(labels)\n",
    "\n",
    "        print('best accuracy: %.4f' % best_step_acc)\n",
    "        torch.save(self.feature_extractor_target.state_dict(), self.feature_extractor_target_pth)\n",
    "        torch.save(self.feature_classifier_target.state_dict(), self.feature_classifier_target_pth)\n",
    "        torch.save(self.feature_generator.state_dict(), self.feature_generator_pth)\n",
    "        torch.save(self.feature_discriminator.state_dict(), self.feature_discriminator_pth)\n",
    "        optimal_attention = self.generator_attention.numpy()\n",
    "        np.save(self.optimal_attention_npy, optimal_attention)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--gpu\", type=int, default=1, help=\" 0, 1, 2 or 3 \")\n",
    "    parser.add_argument(\"--mode\", type=str, default='',\n",
    "                    help=\" 'channel_evaluation', 'train_gan' \")\n",
    "\n",
    "    parser.add_argument(\"--source_dataset\", type=str, default='imagenet',\n",
    "                    help=\" 'emnist', 'cifar', 'imagenet' \")\n",
    "                    # lenet: emnist, vgg: cifar, resnet: imagenet\n",
    "\n",
    "    parser.add_argument(\"--target_dataset\", type=str, default='stl',\n",
    "                    help=\" 'svhn', 'fashion_mnist', 'stl', 'cinic', 'caltech', 'food' \")\n",
    "                    # lenet: svhn, fashion_mnist, vgg: stl, cinic, resnet: caltech, food\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--model_save_path\", type=str, default='./', help='save folder name')\n",
    "\n",
    "    parser.add_argument(\"--generator_step\", type=int, default=40000, help='generator training step')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help='batch_size')\n",
    "    parser.add_argument(\"--minor_class_num\", type=int, default=10, help='minor class num')\n",
    "    parser.add_argument(\"--minor_class_ratio\", type=float, default=0.1, help='minor class len = major_class_len * minor_class_ratio')\n",
    "\n",
    "    parser.add_argument(\"--step_for_3_4\", type=int, default=1, help='3, 4 per step')\n",
    "    parser.add_argument(\"--step_for_5\", type=int, default=5, help='5 per step')\n",
    "    parser.add_argument(\"--step_for_6\", type=int, default=2000, help='6 per step')\n",
    "    parser.add_argument(\"--e_loss_for_6\", type=float, default=0.05, help='minimum e loss for 6')\n",
    "\n",
    "    parser.add_argument(\"--extractor_learning_rate\", type=float, default=1e-4, help='extractor learning rate')\n",
    "    parser.add_argument(\"--classifier_learning_rate\", type=float, default=1e-4, help='classifier learning rate')\n",
    "    parser.add_argument(\"--discriminator_learning_rate\", type=float, default=1e-4, help='discriminator learning rate')\n",
    "    parser.add_argument(\"--generator_learning_rate\", type=float, default=1e-4, help='generator learning rate')\n",
    "\n",
    "    # weight path\n",
    "    parser.add_argument(\"--extractor_weight_path\", type=str, default='', help='extractor weight path')\n",
    "    parser.add_argument(\"--classifier_weight_path\", type=str, default='', help='classifier weight path')\n",
    "    parser.add_argument(\"--generator_weight_path\", type=str, default='', help='generator weight path')\n",
    "    parser.add_argument(\"--optimal_attention_path\", type=str, default='', help='optimal attention path')\n",
    "\n",
    "    parser.add_argument(\"--rho\", type=float, default=0.75, help='rho for generator attention')\n",
    "    parser.add_argument(\"--lambda_gp\", type=float, default=10, help='lambda_gp in wgan gp')\n",
    "    parser.add_argument(\"--alpha_extractor\", type=float, default=0.01, help='alpha for extractor')\n",
    "    parser.add_argument(\"--alpha_classifier\", type=float, default=0.01, help='alpha for classifier')\n",
    "    parser.add_argument(\"--beta_classifier\", type=float, default=0.01, help='beta for classifier')\n",
    "    parser.add_argument(\"--num_d_iters\", type=int, default=5, help='num_d_iters')\n",
    "    parser.add_argument(\"--pretrained_base_model\", type=str, default='', help='pretrained_base_model')\n",
    "\n",
    "    parser.add_argument(\"--with_regularization\", type=str, default='with', help='with regularization')\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if args.with_regularization == 'with':\n",
    "        with_regularization = True\n",
    "        print('with_regularization')\n",
    "    else:\n",
    "        with_regularization = False\n",
    "        print('no_regularization')\n",
    "\n",
    "    if args.pretrained_base_model == '': # '' for imagenet\n",
    "        pretrained_base_model = None\n",
    "    else:\n",
    "        pretrained_base_model = args.pretrained_base_model\n",
    "\n",
    "    test = TrainOps(gpu_num=args.gpu, batch_size=args.batch_size,\n",
    "                    extractor_learning_rate=args.extractor_learning_rate, classifier_learning_rate=args.classifier_learning_rate, discriminator_learning_rate=args.discriminator_learning_rate,\n",
    "                    generator_learning_rate=args.generator_learning_rate, minor_class_num=args.minor_class_num, minor_class_ratio=args.minor_class_ratio, with_regularization=with_regularization,\n",
    "                    model_save_path=args.model_save_path)\n",
    "\n",
    "    if args.mode == 'channel_evaluation':\n",
    "        test.channel_evaluation(pretrained_base_model=pretrained_base_model)\n",
    "    elif args.mode == 'train_gan':\n",
    "        test.train_gan(generator_step=args.generator_step, rho=args.rho, lambda_gp=args.lambda_gp, alpha_extractor=args.alpha_extractor, alpha_classifier=args.alpha_classifier,\n",
    "                        beta_classifier=args.beta_classifier, num_d_iters=args.num_d_iters, e_loss_for_6=args.e_loss_for_6, \n",
    "                        step_for_3_4=args.step_for_3_4, step_for_5=args.step_for_5, \n",
    "                        step_for_6=args.step_for_6, pretrained_base_model=pretrained_base_model)\n",
    "    else:\n",
    "        raise ValueError('Unknown mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4ab88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
