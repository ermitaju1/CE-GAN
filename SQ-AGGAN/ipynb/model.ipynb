{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from TimeDistributedLayer import TimeDistributedConv2d, TimeDistributedMaxPool, TimeDistributedUpsampling\n",
    "from BiConvLSTM import BiConvLSTM\n",
    "\n",
    "###Generator\n",
    "class DeepSequentialNet(nn.Module):\n",
    "    def __init__(self, num_sequence, feature_dim, device):\n",
    "        super(DeepSequentialNet, self).__init__()\n",
    "        self.num_sequence = num_sequence\n",
    "        self.feature_dim = feature_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.encoding_block1 = nn.Sequential(\n",
    "            TimeDistributedConv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),     ### input channel = 1(intensity) + 5(slice number info) = 6 \n",
    "            nn.ELU(inplace=True),\n",
    "            TimeDistributedConv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "        self.encoding_block2 = nn.Sequential(\n",
    "            TimeDistributedConv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False, dropout=True),\n",
    "            nn.ELU(inplace=True),\n",
    "            TimeDistributedConv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False, dropout=True),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "        self.encoding_block3 = nn.Sequential(\n",
    "            TimeDistributedConv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False, dropout=True),\n",
    "            nn.ELU(inplace=True),\n",
    "            TimeDistributedConv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False, dropout=True),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.fc_feature = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, self.num_sequence*256*8*8, bias=False),\n",
    "            nn.BatchNorm1d(self.num_sequence*256*8*8)\n",
    "        )\n",
    "        \n",
    "        self.biCLSTM1 = BiConvLSTM(input_size=(8,8), input_dim=256*2, hidden_dim=512, kernel_size=(3,3), num_layers=3, device=self.device)\n",
    "\n",
    "        self.decoding_block3 = nn.Sequential(\n",
    "            TimeDistributedConv2d(512+256, 256, kernel_size=3, stride=1, padding=1, bias=False, dropout=True),\n",
    "            nn.ELU(inplace=True),\n",
    "            TimeDistributedConv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False, dropout=True),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "        self.decoding_block2 = nn.Sequential(\n",
    "            TimeDistributedConv2d(256+128, 128, kernel_size=3, stride=1, padding=1, bias=False, dropout=True),\n",
    "            nn.ELU(inplace=True),\n",
    "            TimeDistributedConv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False, dropout=True),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "        self.decoding_block1 = nn.Sequential(\n",
    "            TimeDistributedConv2d(128+64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.biCLSTM2 = BiConvLSTM(input_size=(64,64), input_dim=64, hidden_dim=64, kernel_size=(3,3), num_layers=3, device=self.device)\n",
    "        \n",
    "        self.maxpooling = TimeDistributedMaxPool(2, stride=2)\n",
    "        self.upsampling = TimeDistributedUpsampling(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        self.onebyoneConv = nn.Sequential(\n",
    "            nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence, feature):\n",
    "        ### encoding\n",
    "        encoded_vol1 = self.encoding_block1(sequence)\n",
    "        maxpooled_encoded_vol1 = self.maxpooling(encoded_vol1)\n",
    "        encoded_vol2 = self.encoding_block2(maxpooled_encoded_vol1)\n",
    "        maxpooled_encoded_vol2 = self.maxpooling(encoded_vol2)\n",
    "        encoded_vol3 = self.encoding_block3(maxpooled_encoded_vol2)\n",
    "        maxpooled_encoded_vol3 = self.maxpooling(encoded_vol3)\n",
    "\n",
    "        ### feature embedding\n",
    "        feature_code = self.fc_feature(feature)\n",
    "        feature_code = feature_code.view(-1, self.num_sequence, 256, 8, 8)\n",
    "\n",
    "        ### concatenate feature with encoding code\n",
    "        sequence_feature_code = torch.cat((maxpooled_encoded_vol3, feature_code), 2)\n",
    "\n",
    "        ### lSTM\n",
    "        lstm_vol1 = self.biCLSTM1(sequence_feature_code)\n",
    "\n",
    "        ### decoding\n",
    "        up_vol3 = self.upsampling(lstm_vol1)\n",
    "        concat_vol3 = torch.cat((encoded_vol3, up_vol3), 2)\n",
    "        decoded_vol3 = self.decoding_block3(concat_vol3)\n",
    "        up_vol2 = self.upsampling(decoded_vol3)\n",
    "        concat_vol2 = torch.cat((encoded_vol2, up_vol2), 2)\n",
    "        decoded_vol2 = self.decoding_block2(concat_vol2)\n",
    "        up_vol1 = self.upsampling(decoded_vol2)\n",
    "        concat_vol1 = torch.cat((encoded_vol1, up_vol1), 2)\n",
    "        decoded_vol1 = self.decoding_block1(concat_vol1)\n",
    "\n",
    "        ### LSTM\n",
    "        lstm_vol2 = self.biCLSTM2(decoded_vol1)\n",
    "        lstm_vol2 = torch.sum(lstm_vol2, 1)\n",
    "\n",
    "        ### Last PANG!~\n",
    "        synth_code = self.onebyoneConv(lstm_vol2)\n",
    "\n",
    "        # print(\"encoded_vol1 : \", encoded_vol1.shape)\n",
    "        # print(\"maxpooled_encoded_vol1 : \", maxpooled_encoded_vol1.shape)\n",
    "        # print(\"encoded_vol2 : \", encoded_vol2.shape)\n",
    "        # print(\"maxpooled_encoded_vol1 : \", maxpooled_encoded_vol2.shape)\n",
    "        # print(\"encoded_vol3 : \", encoded_vol3.shape)\n",
    "        # print(\"maxpooled_encoded_vol1 : \", maxpooled_encoded_vol3.shape)\n",
    "        # print(\"lstm_vol1 : \", lstm_vol1.shape)\n",
    "        # print(\"up_vol3 : \", up_vol3.shape)\n",
    "        # print(\"concat_vol3 : \", concat_vol3.shape)\n",
    "        # print(\"decoded_vol3 : \", decoded_vol3.shape)\n",
    "        # print(\"up_vol2 : \", up_vol2.shape)\n",
    "        # print(\"concat_vol2 : \", concat_vol2.shape)\n",
    "        # print(\"decoded_vol2 : \", decoded_vol2.shape)\n",
    "        # print(\"up_vol1 : \", up_vol1.shape)\n",
    "        # print(\"concat_vol1 : \", concat_vol1.shape)\n",
    "        # print(\"decoded_vol1 : \", decoded_vol1.shape)\n",
    "        # print(\"lstm_vol2 : \", lstm_vol2.shape)\n",
    "        # print(\"synth_code : \", synth_code.shape)\n",
    "\n",
    "        return synth_code\n",
    "\n",
    "\n",
    "#in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1, bias=False),            ### 64 -> 32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 32 * 2, 4, 2, 1, bias=False),       ### 32 -> 16\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(32 * 2, 32 * 4, 4, 2, 1, bias=False),   ### 16 -> 8\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "            nn.Conv2d(32 * 4, 32 * 8, 4, 2, 1, bias=False),   ### 8 -> 4\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(32 * 8, 1, 4, 2, 1, bias=False),   ### 4 -> 4\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.feature_fc = nn.Sequential(\n",
    "            nn.Linear(1*2*2, self.feature_dim, bias=False)\n",
    "            \n",
    "        )\n",
    "        self.yn_fc = nn.Sequential(\n",
    "            nn.Linear(1*2*2, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, pred_slice):\n",
    "        output = self.encoder(pred_slice)     ### (b, 1, 2, 2)\n",
    "        \n",
    "        output_flatten = output.view(pred_slice.shape[0], -1)\n",
    "        cl_feature = self.feature_fc(output_flatten)\n",
    "        cl_yn = self.yn_fc(output_flatten)\n",
    "        return output, cl_feature, cl_yn.view(-1)\n",
    "\n",
    "\n",
    "######cl_feature cl_yn의 역할이 뭘까\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
