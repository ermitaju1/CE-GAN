{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9fc4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import PRIO_PGRP\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from medpy.io import load, save\n",
    "import cv2\n",
    "\n",
    "from model import DeepSequentialNet, Discriminator\n",
    "\n",
    "\n",
    "LAMBDA = 10\n",
    "CRITIC_ITERS = 5\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__  ##m의 클래스 이름을 가져옴 m이 어디 소속이냐\n",
    "    if isinstance(m, nn.Conv2d):  #isinstance는 m이 nn.Conv2D인지를 묻는거\n",
    "        nn.init.xavier_uniform_(m.weight.data) ##xavier는 uniform이냐 normal이냐 2개 종류로 초기화 -> w를 균일분포로 초기화\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.normal_(1.0, 0.02) #정규분포로 초기화 \n",
    "        m.bias.data.fill_(0)\n",
    "    #conv이면 균일분포로, batchnormalization은 정규분포로 weight값 초기화\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data, device):\n",
    "    alpha = torch.rand(real_data.size()[0], 1, 1, 1) ###1개를 4차원으로 \n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.to(device)  #GPU가 계산해라\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)  ##오차 줄이는 작업\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True) ##옛날 방식\n",
    "    #interpolates = torch.tensor(interplates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates,_,_ = netD(interpolates)\n",
    "\n",
    "    ##이것도 수정할 수 있을텐데 어케 하는걸까\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1) + 1e-16\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "class sequentialSynthesisTrainer(object):\n",
    "    def __init__(self, writer, epochs, gpu, batch_size, d_learning_rate, g_learning_rate, num_sequence, image_size, feature_dim, nodule_lambda, background_lambda, feature_lambda, yn_lambda, output_dir, train_dataloader, test_dataloader):\n",
    "        self.model_dir = os.path.join(output_dir, 'model')\n",
    "        self.train_result_dir = os.path.join(output_dir, 'result', 'train')\n",
    "        self.test_result_dir = os.path.join(output_dir, 'result', 'test')\n",
    "        os.makedirs(self.model_dir)\n",
    "        os.makedirs(self.train_result_dir)\n",
    "        os.makedirs(self.test_result_dir)\n",
    "\n",
    "        self.writer = writer\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.gpu = gpu\n",
    "        self.device = torch.device(\"cuda:%s\" % self.gpu)\n",
    "       \n",
    "        self.batch_size = batch_size\n",
    "        self.d_learning_rate = d_learning_rate\n",
    "        self.g_learning_rate = g_learning_rate\n",
    "\n",
    "        self.num_sequence = num_sequence\n",
    "        self.image_size = image_size\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.nodule_lambda = nodule_lambda\n",
    "        self.background_lambda = background_lambda\n",
    "        self.feature_lambda = feature_lambda\n",
    "        self.yn_lambda = yn_lambda\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        \n",
    "    def train(self):\n",
    "        netG = DeepSequentialNet(num_sequence=self.num_sequence, feature_dim=self.feature_dim, device=self.device).to(self.device)\n",
    "        netD = Discriminator(feature_dim=self.feature_dim).to(self.device)\n",
    "        netG.apply(weights_init)\n",
    "        netD.apply(weights_init)\n",
    "        # netG.load_state_dict(torch.load('/home/sojeong/LSTM_Synthesis2/experiments/2020_12_08_09_49_0th_fold_test_dropout/model/netG_final.pth', map_location=lambda storage, loc: storage))\n",
    "        # netD.load_state_dict(torch.load('/home/sojeong/LSTM_Synthesis2/experiments/2020_12_08_09_49_0th_fold_test_dropout/model/netD_final.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "        global errG_list, errD_list\n",
    "        errG_list = []\n",
    "        errD_list = []\n",
    "        print(\"********************************************netG********************************************\")\n",
    "        print(netG)\n",
    "        print(\"********************************************netD********************************************\")\n",
    "        print(netD)\n",
    "        print(\"********************************************************************************************\")\n",
    "        print(\"********************************************************************************************\")\n",
    "        print(\"********************************************************************************************\")\n",
    "\n",
    "        ### optimizer, loss func\n",
    "        optimizerG = optim.Adam(netG.parameters(), lr=self.g_learning_rate, betas=(0.5, 0.999))\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=self.d_learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "        MSE_criterion = nn.MSELoss().to(self.device)\n",
    "        L1_criterion = nn.L1Loss().to(self.device)\n",
    "        BCE_criterion = nn.BCEWithLogitsLoss().to(self.device)\n",
    "\n",
    "        # real, fake label\n",
    "        real_labels = Variable(torch.FloatTensor(self.batch_size).fill_(1)).to(self.device)\n",
    "        fake_labels = Variable(torch.FloatTensor(self.batch_size).fill_(0)).to(self.device)\n",
    "\n",
    "        total_step = 0\n",
    "        result_print_step = 100\n",
    "        start_time_step = time.time()\n",
    "        \n",
    "        for epoch in range(0, self.epochs):\n",
    "            if epoch == 50 :\n",
    "                result_print_step = 500\n",
    "\n",
    "            start_time_epoch = time.time()\n",
    "            for data in self.train_dataloader:\n",
    "                netG.train()\n",
    "                netD.train()\n",
    "                ### data preparation\n",
    "                # masked_vol_sequence, masked_bg_sequence, bg_sequence, feature_sequence, real_slice, nodule_mask, bg_mask = data\n",
    "                masked_vol_sequence, bg_sequence, feature_sequence, real_slice, nodule_mask, bg_mask = data\n",
    "                masked_vol_sequence = Variable(masked_vol_sequence).float().to(self.device)\n",
    "                # masked_bg_sequence = Variable(masked_bg_sequence).float().to(self.device)\n",
    "                bg_sequence = Variable(bg_sequence).float().to(self.device)\n",
    "                feature_sequence = Variable(feature_sequence).float().to(self.device)\n",
    "                real_slice = Variable(real_slice).float().to(self.device)\n",
    "                nodule_mask = Variable(nodule_mask).float().to(self.device)\n",
    "                bg_mask = Variable(bg_mask).float().to(self.device)\n",
    "\n",
    "                ### generate fake one\n",
    "                netG.requires_grad_(True)\n",
    "                fake_slice1 = netG(masked_vol_sequence, feature_sequence)\n",
    "                fake_slice2 = netG(bg_sequence, feature_sequence)\n",
    "                # fake_slice2 = netG(masked_bg_sequence, feature_sequence)\n",
    "\n",
    "                ##########################\n",
    "                ### update discriminator\n",
    "                ##########################\n",
    "                netD.requires_grad_(True)\n",
    "                real_logit1, real_feature1, real_yn1 = netD(real_slice)     \n",
    "                real_logit2, real_feature2, real_yn2 = netD(bg_sequence[:,1,:,:,:])     \n",
    "                fake_logit1, fake_feature1, fake_yn1 = netD(fake_slice1.detach())\n",
    "                fake_logit2, fake_feature2, fake_yn2 = netD(fake_slice2.detach())\n",
    "                gradient_penalty1 = calc_gradient_penalty(netD, real_slice, fake_slice1.detach(), self.device)\n",
    "                gradient_penalty2 = calc_gradient_penalty(netD, bg_sequence[:,1,:,:,:], fake_slice2.detach(), self.device)   ### 여기\n",
    "\n",
    "                errD_wgan_gp1 = -torch.mean(real_logit1) + torch.mean(fake_logit1) + gradient_penalty1 \n",
    "                errD_wgan_gp2 = -torch.mean(real_logit2) + torch.mean(fake_logit2) + gradient_penalty2 \n",
    "                errD_feature = MSE_criterion(real_feature1, feature_sequence) + MSE_criterion(fake_feature1, feature_sequence) + MSE_criterion(fake_feature2, feature_sequence)\n",
    "                errD_yn = BCE_criterion(real_yn1, real_labels) + BCE_criterion(real_yn2, fake_labels) + BCE_criterion(fake_yn1, real_labels) + BCE_criterion(fake_yn2, real_labels)\n",
    "                \n",
    "                errD = errD_wgan_gp1 + errD_wgan_gp2 + self.feature_lambda * errD_feature + self.yn_lambda * errD_yn\n",
    "                print(errD)\n",
    "                errD_list.append(errD.item())\n",
    "                \n",
    "                netD.zero_grad()\n",
    "                errD.backward()\n",
    "                optimizerD.step()  \n",
    "                netD.requires_grad_(False)\n",
    "\n",
    "                ##########################\n",
    "                ### update generator\n",
    "                ##########################\n",
    "                errG = 0\n",
    "                errG_nodule = 0\n",
    "                errG_bg = 0\n",
    "                errG_feature = 0\n",
    "                errG_yn = 0\n",
    "                if total_step % CRITIC_ITERS == 0:\n",
    "                    pred_logit1, pred_feature1, pred_yn1 = netD(fake_slice1)\n",
    "                    pred_logit2, pred_feature2, pred_yn2 = netD(fake_slice2)\n",
    "                    \n",
    "                    cnt_nodule = torch.count_nonzero(nodule_mask) \n",
    "                    errG_nodule = (L1_criterion(fake_slice1*nodule_mask, real_slice*nodule_mask) + L1_criterion(fake_slice2*nodule_mask, real_slice*nodule_mask)) / cnt_nodule * (self.image_size*self.image_size*self.batch_size)\n",
    "                    errG_bg = L1_criterion(fake_slice1*bg_mask, real_slice*bg_mask) + L1_criterion(fake_slice2*bg_mask, bg_sequence[:,1,:,:,:]*bg_mask)\n",
    "                    errG_feature = MSE_criterion(pred_feature1, feature_sequence) + MSE_criterion(pred_feature2, feature_sequence)\n",
    "                    errG_yn = BCE_criterion(pred_yn1, real_labels) + BCE_criterion(pred_yn2, real_labels) \n",
    "                    \n",
    "                \n",
    "                    errG = - torch.mean(pred_logit1) - torch.mean(pred_logit2) + self.nodule_lambda * errG_nodule + self.background_lambda * errG_bg + self.feature_lambda * errG_feature + self.yn_lambda * errG_yn\n",
    "                    print(errG)\n",
    "                    errG_list.append(errG.item())\n",
    "                \n",
    "                    \n",
    "                    netG.zero_grad()\n",
    "                    errG.backward()\n",
    "                    optimizerG.step()\n",
    "                    netG.requires_grad_(False)\n",
    "            \n",
    "\n",
    "                if total_step % 100 == 0:\n",
    "                    end_time_step = time.time()  \n",
    "                    \n",
    "                    ### print loss\n",
    "                    print('[%d / %d - %d step] errD : %.5f  errG : %.5f  err_nodules : %.5f err_backgrounds : %.5f  time : %.2fs' % (epoch, self.epochs, total_step, errD.item(), errG.item(), errG_nodule.item(), errG_bg.item(),  end_time_step - start_time_step))                \n",
    "                    \n",
    "                    self.writer.add_scalar('train_loss/errD', errD.item(), total_step)\n",
    "                    self.writer.add_scalar('train_loss/errD_wgan_gp1', errD_wgan_gp1.item(), total_step)\n",
    "                    self.writer.add_scalar('train_loss/errD_wgan_gp2', errD_wgan_gp2.item(), total_step)\n",
    "                    self.writer.add_scalar('train_loss/errD_feature', errD_feature.item(), total_step)\n",
    "                    self.writer.add_scalar('train_loss/errD_yn', errD_yn.item(), total_step)\n",
    "\n",
    "\n",
    "                    self.writer.add_scalar('train_loss/errG', errG.item(), total_step)\n",
    "                    self.writer.add_scalar('train_loss/err_nodules', errG_nodule.item(), total_step)\n",
    "                    self.writer.add_scalar('train_loss/err_backgrounds', errG_bg.item(), total_step)\n",
    "                    self.writer.add_scalar('train_loss/errG_feature', errG_feature.item(), total_step)\n",
    "                    self.writer.add_scalar('train_loss/errG_yn', errG_yn.item(), total_step)\n",
    "\n",
    "\n",
    "                    ## print result image\n",
    "                    if total_step % result_print_step == 0:\n",
    "                        netG.eval()\n",
    "                        netD.eval()\n",
    "                        netG.requires_grad_(False)\n",
    "                        netD.requires_grad_(False)\n",
    "                        with torch.no_grad():\n",
    "                            fake_slice1 = fake_slice1.cpu().numpy()\n",
    "                            fake_slice1 = np.squeeze(fake_slice1)\n",
    "\n",
    "                            fake_slice2 = fake_slice2.cpu().numpy()\n",
    "                            fake_slice2 = np.squeeze(fake_slice2)\n",
    "\n",
    "                            real_slice = real_slice.cpu().numpy()\n",
    "                            real_slice = np.squeeze(real_slice)\n",
    "\n",
    "                            bg_sequence = bg_sequence[:,1,:,:,:].cpu().numpy()\n",
    "                            bg_sequence = np.squeeze(bg_sequence)\n",
    "\n",
    "                            masked_vol_sequence = masked_vol_sequence.cpu().numpy()\n",
    "                            masked_vol_sequence = np.transpose(masked_vol_sequence, (0, 2, 3, 4, 1))   ###(b,c,x,y,s)\n",
    "                            masked_vol_sequence = masked_vol_sequence[:, 0, :, :, 1]\n",
    "\n",
    "                            nodule_mask = nodule_mask.cpu().numpy()\n",
    "                            nodule_mask = np.where(nodule_mask==1, 255, nodule_mask)\n",
    "                            nodule_mask = np.squeeze(nodule_mask)\n",
    "                            bg_mask = bg_mask.cpu().numpy()\n",
    "                            bg_mask = np.where(bg_mask==1, 255, bg_mask)\n",
    "                            bg_mask = np.squeeze(bg_mask)\n",
    "\n",
    "                            result_imgs = np.array([])\n",
    "                            for train_result_idx, (gt, seq1, pred1, seq2, pred2, nm, bgm) in enumerate(zip(real_slice, masked_vol_sequence, fake_slice1, bg_sequence, fake_slice2, nodule_mask, bg_mask)):\n",
    "                                gt = gt*255\n",
    "                                seq1 = seq1*255\n",
    "                                pred1 = pred1*255\n",
    "                                seq2 = seq2*255\n",
    "                                pred2 = pred2*255\n",
    "\n",
    "                                result_img = np.concatenate((gt, seq1, pred1, seq2, pred2, nm, bgm), 1)\n",
    "\n",
    "                                if train_result_idx == 0:\n",
    "                                    result_imgs = result_img\n",
    "                                    \n",
    "                                else:\n",
    "                                    result_imgs = np.concatenate((result_imgs, result_img), 0)\n",
    "                            \n",
    "                                if train_result_idx==6:\n",
    "                                    break\n",
    "                            cv2.imwrite(self.train_result_dir + '/epoch_' + str(epoch) + '_step' + str(total_step) +'.png', result_imgs)\n",
    "                            \n",
    "                        netG.train()\n",
    "                        netD.train()\n",
    "                        print(errG)\n",
    "                        print(errD)\n",
    "                    start_time_step = time.time()\n",
    "\n",
    "                total_step += 1\n",
    "            ###################################error##########\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "        \n",
    "            end_time_epoch = time.time()\n",
    "            \n",
    "            print('[%d / %d - %d step] training time : %.5fs ' % (epoch, self.epochs, total_step, end_time_epoch-start_time_epoch))\n",
    "            #print(result_img)\n",
    "            ##########################################################\n",
    "            ##########################################################\n",
    "            ##########################################################\n",
    "            netG.eval()\n",
    "            for m in netG.modules():\n",
    "                if m.__class__.__name__.startswith('Dropout'):\n",
    "                    m.train()\n",
    "            netD.eval()\n",
    "            for m in netD.modules():\n",
    "                if m.__class__.__name__.startswith('Dropout'):\n",
    "                    m.train()\n",
    "            netG.requires_grad_(False)\n",
    "            netD.requires_grad_(False)\n",
    "            \n",
    "        \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data in self.test_dataloader:\n",
    "                    # test_masked_vol_sequence, test_masked_bg_sequence, test_bg_sequence, test_feature_sequence, test_real_slice, nodule_mask, bg_mask = data\n",
    "                    test_masked_vol_sequence, test_bg_sequence, test_feature_sequence, test_real_slice, nodule_mask, bg_mask = data\n",
    "                    test_masked_vol_sequence = Variable(test_masked_vol_sequence).float().to(self.device)\n",
    "                    # test_masked_bg_sequence = Variable(test_masked_bg_sequence).float().to(self.device)\n",
    "                    test_bg_sequence = Variable(test_bg_sequence).float().to(self.device)\n",
    "                    test_feature_sequence = Variable(test_feature_sequence).float().to(self.device)\n",
    "                    test_real_slice = Variable(test_real_slice).float().to(self.device)\n",
    "                    nodule_mask = Variable(nodule_mask).float().to(self.device)\n",
    "                    bg_mask = Variable(bg_mask).float().to(self.device)\n",
    "                    \n",
    "                    test_fake_slice1 = netG(test_masked_vol_sequence, test_feature_sequence)\n",
    "                    test_fake_slice2 = netG(test_bg_sequence, test_feature_sequence)\n",
    "                    # test_fake_slice2 = netG(test_masked_bg_sequence, test_feature_sequence)\n",
    "\n",
    "                    #######################################################\n",
    "                    #######################################################  \n",
    "                    fake_logit1, fake_feature1, fake_yn1 = netD(test_fake_slice1)\n",
    "                    fake_logit2, fake_feature2, fake_yn2 = netD(test_fake_slice2)\n",
    "                    \n",
    "                    cnt_nodule = torch.count_nonzero(nodule_mask) \n",
    "                    test_errG_nodule = (L1_criterion(test_fake_slice1*nodule_mask, test_real_slice*nodule_mask) + L1_criterion(test_fake_slice2*nodule_mask, test_real_slice*nodule_mask)) / cnt_nodule * (self.image_size*self.image_size*self.batch_size)\n",
    "                    test_errG_bg = L1_criterion(test_fake_slice1*bg_mask, test_real_slice*bg_mask) + L1_criterion(test_fake_slice2*bg_mask, test_bg_sequence[:,1,:,:,:]*bg_mask) \n",
    "                    test_errG_feature = MSE_criterion(fake_feature1, test_feature_sequence) + MSE_criterion(fake_feature2, test_feature_sequence)\n",
    "                    test_errG_yn = BCE_criterion(fake_yn1, real_labels) + BCE_criterion(fake_yn2, real_labels)    ### .......... hahahah....h......ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 욕먹어도 싸다\n",
    "                    \n",
    "                    test_errG = - torch.mean(fake_logit1) - torch.mean(fake_logit2) + self.nodule_lambda * test_errG_nodule + self.background_lambda * test_errG_bg + test_errG_feature + test_errG_yn\n",
    "            \n",
    "                    #####################################이게 print 되어야 하는거 아닌가?????우앵\n",
    "                    print('[%d / %d - %d step] errG : %.5f  err_nodules : %.5f err_backgrounds : %.5f ' % (epoch, self.epochs, total_step, test_errG.item(), test_errG_nodule.item(), test_errG_bg.item()))\n",
    "                    self.writer.add_scalar('test_loss/test_errG', test_errG.item(), epoch)\n",
    "                    self.writer.add_scalar('test_loss/test_err_nodule', test_errG_nodule.item(), epoch)\n",
    "                    self.writer.add_scalar('test_loss/test_err_background', test_errG_bg.item(), epoch)\n",
    "                    self.writer.add_scalar('test_loss/test_errG_feature', test_errG_feature.item(), epoch)\n",
    "                    self.writer.add_scalar('test_loss/test_errG_yn', test_errG_yn.item(), epoch)\n",
    "\n",
    "                    #######################################################\n",
    "                    #######################################################\n",
    "                    test_fake_slice1 = test_fake_slice1.cpu().numpy()\n",
    "                    test_fake_slice1 = np.squeeze(test_fake_slice1)\n",
    "\n",
    "                    test_fake_slice2 = test_fake_slice2.cpu().numpy()\n",
    "                    test_fake_slice2 = np.squeeze(test_fake_slice2)\n",
    "\n",
    "                    test_real_slice = test_real_slice.cpu().numpy()\n",
    "                    test_real_slice = np.squeeze(test_real_slice)\n",
    "\n",
    "                    test_bg_sequence = test_bg_sequence.cpu().numpy()\n",
    "                    test_bg_sequence = np.transpose(test_bg_sequence, (0, 2, 3, 4, 1))                   ###(b,c,x,y,s)\n",
    "                    test_bg_sequence = test_bg_sequence[:, 0, :, :, 1]\n",
    "\n",
    "                    test_masked_vol_sequence = test_masked_vol_sequence.cpu().numpy()\n",
    "                    test_masked_vol_sequence = np.transpose(test_masked_vol_sequence, (0, 2, 3, 4, 1))   ###(b,c,x,y,s)\n",
    "                    test_masked_vol_sequence = test_masked_vol_sequence[:, 0, :, :, 1]\n",
    "\n",
    "                    nodule_mask = nodule_mask.cpu().numpy()\n",
    "                    nodule_mask = np.where(nodule_mask==1, 255, nodule_mask)\n",
    "                    nodule_mask = np.squeeze(nodule_mask)\n",
    "                    bg_mask = bg_mask.cpu().numpy()\n",
    "                    bg_mask = np.where(bg_mask==1, 255, bg_mask)\n",
    "                    bg_mask = np.squeeze(bg_mask)\n",
    "\n",
    "                    result_imgs = np.array([])\n",
    "                    for idx, (gt, seq1, pred1, seq2, pred2, nm, bgm) in enumerate(zip(test_real_slice, test_masked_vol_sequence, test_fake_slice1, test_bg_sequence, test_fake_slice2, nodule_mask, bg_mask)):\n",
    "                        gt = gt*255\n",
    "                        seq1 = seq1*255\n",
    "                        pred1 = pred1*255\n",
    "                        seq2 = seq2*255\n",
    "                        pred2 = pred2*255\n",
    "\n",
    "                        result_img = np.concatenate((gt, seq1, pred1, seq2, pred2, nm, bgm), 1)\n",
    "\n",
    "\n",
    "                        if idx == 0:\n",
    "                            result_imgs = result_img\n",
    "                        else:\n",
    "                            result_imgs = np.concatenate((result_imgs, result_img), 0)\n",
    "                    \n",
    "                        if idx==6:\n",
    "                            break\n",
    "                    cv2.imwrite(self.test_result_dir + '/epoch_' + str(epoch) + '_step' + str(total_step) +'.png', result_imgs)\n",
    "                    break  \n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                torch.save(netG.state_dict(), '%s/netG_epoch%d.pth' % (self.model_dir, epoch))\n",
    "                torch.save(netD.state_dict(), '%s/netD_epoch%d.pth' % (self.model_dir, epoch))\n",
    "        torch.save(netG.state_dict(), '%s/netG_final.pth' % (self.model_dir))\n",
    "        torch.save(netD.state_dict(), '%s/netD_final.pth' % (self.model_dir))\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(errG_list)\n",
    "        plt.plot(errD_list)\n",
    "\n",
    "            \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
